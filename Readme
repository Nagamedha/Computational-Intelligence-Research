---
# Smart Sentiment Analytics: Decoding Social Media Emotions with Computational Intelligence

This project analyzes real-world social media content (tweets, YouTube comments) to detect sentiment using multiple machine learning models. From traditional TF-IDF models to transformer-based DistilBERT, we explore the impact of contextual understanding on emotional classification.

---
```markdown

## ğŸ“ Folder Structure

CI research project/
â”‚
â”œâ”€â”€ Datasets/
â”‚   â”œâ”€â”€ training.1600000.processed.noemoticon.csv      # Twitter (Sentiment140)
â”‚   â”œâ”€â”€ US_comments_Cleaned.csv                        # YouTube dataset
â”‚   â””â”€â”€ kaggle.json                                     # Replace with your own Kaggle API key
â”‚
â”œâ”€â”€ Source_Code/
â”‚   â”œâ”€â”€ 1_Traditional_vs_CNN_vs_DistilBERT.ipynb       # All 3 model evaluations + graph
â”‚   â”œâ”€â”€ 2_TFIDF_vs_DistilBERT_Analysis.ipynb           # Graph + metrics table
â”‚   â”œâ”€â”€ 3_DistilBERT_Final_HighAccuracy.ipynb          # 87% model with confusion matrix, dist.
â”‚   â””â”€â”€ AdditionalWork_for_Accuracy/
â”‚       â”œâ”€â”€ Accuracy_83.ipynb                          # High-performing DistilBERT run
â”‚       â””â”€â”€ Accuracy_87.ipynb                          # Matches final validation results
â”‚
â”œâ”€â”€ SoftwareManual/
â”‚   â”œâ”€â”€ README.py                                       # â† Steps to execute the project
â”‚   â””â”€â”€ requirements.txt                                # Pre requisites to download
â”‚
â”œâ”€â”€ Final_Report.pdf                                    # IEEE-style report
â””â”€â”€ Project_Presentation_CI_Medha.pptx                  # PPT
```

---

## What This Project Does

- Fine-tunes transformer models (DistilBERT) on noisy, emotion-rich social content
- Benchmarks traditional models (TF-IDF + LR, CNN)
- Handles emojis, slang, sarcasm via preprocessing
- Visualizes distribution, word clouds, and confusion matrix
- Achieves **87% accuracy** and **F1-score of 0.87** with DistilBERT

---

## How to Run the Code

### Step 1: Install Dependencies

Use the provided `requirements.txt`:

```bash
pip install -r SoftwareManual/requirements.txt
```

### Step 2: Use Your Own Kaggle API

- Replace the `kaggle.json` file with **your own Kaggle credentials**
- Place it in `Datasets/` or your notebook's working directory

Generate yours from:  
https://www.kaggle.com/settings/account â†’ "Create New API Token"

### Step 3: Open and Execute Notebooks

Launch each `.ipynb` in `Source_Code/` via:

- **Google Colab** (recommended for DistilBERT)
- Or **Jupyter Lab** locally

Start with:
- `1_Traditional_vs_CNN_vs_DistilBERT.ipynb`
- Then: `2_TFIDF_vs_DistilBERT_Analysis.ipynb`
- Then: `3_DistilBERT_Final_HighAccuracy.ipynb`
- Lastly: Optional deeper work â†’ `AdditionalWork_for_Accuracy/`

---

## Results Summary

| Model                      | Accuracy | F1 Score |
|---------------------------|----------|----------|
| TF-IDF + LogisticRegression | 71.8%   | 0.72     |
| CNN                       | 65.4%   | 0.65     |
| DistilBERT (final)        | 87.0%   | 0.87     |

- All metrics and plots (confusion matrix, sentiment dist.) are included in notebooks

---

## Additional Notes

- Every code file maps directly to results/observations shown in the report and slides
- Files in `AdditionalWork_for_Accuracy/` prove experimentation consistency (83%, 87%)
- All results are backed by **real execution**, not synthetic metrics
- All preprocessing is **custom-built** for social data (emojis, slang, short-form)

---

## Limitations

- Sarcasm detection not deeply integrated (explored via future work)
- Large CSVs excluded from GitHub due to LFS quota, but included in this drive
- All code is built for batch inference, not real-time APIs

---

## Submission Link (Google Drive)

**Project Folder**  
[CI Project Google Drive]:
https://drive.google.com/drive/folders/1uaNAeurbdx-1zIOPtFrP-ugq1_MPKTD6?usp=drive_link

---

## Contributors

- Nagamedha Sakhamuri  
- Nikitha Bonthala  
Under the guidance of: Prof. Yanqing Zhang, Georgia State University

---
